# ğŸ§  DS542 Deep Learning for Data Science â€” Midterm Challenge (Spring 2025)

### ğŸ“ Author: Viktoria Zruttova  
ğŸ“… Date: March 27, 2025  
ğŸ† Kaggle Username: *Viktoria Zruttova*  
ğŸ“Š Best Kaggle Score: **0.57876** (Part 3)

---

## ğŸ“Œ Overview

This repository contains my submission for the DS542 Midterm Challenge, which focuses on image classification using the CIFAR-100 dataset. The challenge was divided into three parts, each increasing in complexity:

1. **Part 1 â€“ Simple CNN:** Design and train a baseline convolutional neural network from scratch.
2. **Part 2 â€“ More Sophisticated CNN (ResNet18):** Train a deeper model and introduce basic regularization and augmentation.
3. **Part 3 â€“ Transfer Learning (ResNet50):** Fine-tune a pretrained model using advanced training techniques and optimization strategies.

---

## ğŸ¤– AI Disclosure

I used the free version of **ChatGPT** for assistance throughout this project. The AI helped me understand CNN architectures, implement regularization techniques, and debug training loops. I personally wrote the initial model definitions, training logic, and hyperparameter tuning strategies, while AI support was primarily used for learning, validation, and code refinement. All code includes detailed comments describing the functionality.

---

## ğŸ“¦ Project Structure

```bash
.
â”œâ”€â”€ part1_simple_cnn.py         # Code for baseline CNN model
â”œâ”€â”€ part2_resnet18.py           # Code for training ResNet18 from scratch
â”œâ”€â”€ part3_resnet50_transfer.py  # Code for fine-tuning pretrained ResNet50
â”œâ”€â”€ utils/                      # Helper functions (data loaders, transforms, etc.)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                   # This file

---

## ğŸ§  Model Summary

### ğŸ”¹ Part 1: Simple CNN  
- **Architecture:** 3 Conv layers + FC layer  
- **Optimizer:** SGD with momentum  
- **Scheduler:** StepLR  
- **Regularization:** None  
- **Kaggle Score:** **0.16158**

### ğŸ”¹ Part 2: ResNet18  
- **Modifications:** Custom classifier head with dropout  
- **Data Augmentation:** Horizontal flip, crop, color jitter  
- **Optimizer:** AdamW + CosineAnnealingLR  
- **Regularization:** Dropout (p=0.5), weight decay (1e-4)  
- **Kaggle Score:** **0.30093**

### ğŸ”¹ Part 3: ResNet50 (Pretrained)  
- **Transfer Learning:** Fine-tuned last 50 parameters  
- **Data Augmentation:** Strong augmentation + Mixup  
- **Regularization:** Dropout, BatchNorm, Label Smoothing (0.1), Early Stopping  
- **Optimizer:** AdamW  
- **Scheduler:** Warmup + CosineAnnealingWarmRestarts  
- **Kaggle Score:** **0.57876**

---

## âš™ï¸ Hyperparameter Tuning

| Parameter         | Value       | Reasoning |
|------------------|-------------|-----------|
| Batch Size       | 32          | Balanced compute and performance |
| Learning Rate    | 0.001       | Empirically best after testing |
| Epochs           | 200         | Early stopping applied (patience=40) |
| Optimizer        | AdamW       | Combines adaptive learning with regularization |
| Scheduler        | Warmup + Cosine | Helps escape local minima and stabilize learning |

---

## ğŸ§ª Regularization & Augmentation Techniques

- **Dropout (0.5):** Reduces overfitting in FC layers  
- **Weight Decay (1e-4):** Penalizes large weights  
- **Early Stopping:** Monitors validation loss to prevent overfitting  
- **Label Smoothing (0.1):** Reduces overconfidence in predictions  
- **Gradient Clipping (max norm=1.0):** Prevents exploding gradients  
- **Data Augmentation:**  
  - Basic: Cropping, flipping, color jitter  
  - Advanced: Random erasing, Mixup blending for improved robustness

---

## ğŸ“Š Experiment Tracking (W&B)

All experiments were tracked using **Weights & Biases**, including:
- Training & validation accuracy/loss
- Learning rate trends
- Early stopping behavior
- Model checkpoints  
These visualizations helped identify overfitting and monitor performance improvements across training iterations.

---

## ğŸ“ˆ Results

| Part | Model         | Kaggle Score |
|------|---------------|--------------|
| 1    | Simple CNN    | 0.16158      |
| 2    | ResNet18      | 0.30093      |
| 3    | ResNet50 (TL) | 0.57876      |

Part 3 achieved the best performance due to its deep architecture, transfer learning, strong regularization, and carefully tuned augmentation strategies.

---

## ğŸš€ How to Run

\`\`\`bash
# Set up virtual environment
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Run each model script independently
python part1_simple_cnn.py
python part2_resnet18.py
python part3_resnet50_transfer.py
\`\`\`

---

## ğŸ“¬ Acknowledgments

Thanks to the DS542 instructors for the structured challenge and to **Weights & Biases** for the free experiment tracking tools.  
Also, a shoutout to ChatGPT for debugging help and support in exploring deep learning concepts!
